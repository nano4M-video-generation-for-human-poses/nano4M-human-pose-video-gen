{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e32d9a3-a332-405b-a740-493f2c95e2f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f3cf2c62e30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from fourm.vq.vqvae import VQVAE, DiVAE\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import scipy.io\n",
    "import cv2\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "from fourm.data.modality_transforms import RGBTransform\n",
    "from fourm.utils import denormalize, IMAGENET_INCEPTION_MEAN, IMAGENET_INCEPTION_STD, IMAGENET_DEFAULT_STD, IMAGENET_DEFAULT_MEAN\n",
    "# The flag below controls whether to allow TF32 on matmul. This flag defaults to False in PyTorch 1.12 and later.\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "# The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "913470e3-ec69-4daf-9225-d33a19e8626e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e4821e1d250482bba00e0520d646d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 7 files:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from cosmos_tokenizer.image_lib import ImageTokenizer\n",
    "snapshot_download(\n",
    "    repo_id='nvidia/Cosmos-0.1-Tokenizer-DI16x16', \n",
    "    local_dir='/tmp/nvidiaxxxx/Cosmos-0.1-Tokenizer-DI16x16'\n",
    ")\n",
    "image_tokenizer = ImageTokenizer(\n",
    "    checkpoint_enc='/tmp/nvidiaxxxx/Cosmos-0.1-Tokenizer-DI16x16/encoder.jit',\n",
    "    checkpoint_dec='/tmp/nvidiaxxxx/Cosmos-0.1-Tokenizer-DI16x16/decoder.jit',\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a46f5401-43f3-409d-8274-bc32190f7fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bd2fd1a-1355-4456-b84e-818b42ca9e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 640, 640])\n",
      "a man standing on a tennis court holding a tennis racket\n",
      "tensor([[30522,  1037,  2158,  3061,  2006,  1037,  5093,  2457,  3173,  1037,\n",
      "          5093, 14513,  3388,   102]])\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('penn_action_raw/Penn_Action/frames/0001/000001.jpg').resize((640,640))\n",
    "img_tensor = TF.to_tensor(img).to(device).unsqueeze(0) * 2 - 1\n",
    "print(img_tensor.shape)\n",
    "inputs = processor(images=img, return_tensors=\"pt\")\n",
    "\n",
    "# Generate caption\n",
    "out = model.generate(**inputs)\n",
    "caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "print(caption)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "442612a6-687e-45cd-b189-21e68b6fea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 14])\n"
     ]
    }
   ],
   "source": [
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb069147-ad7d-4c6f-8f3d-79374e2377b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
