# Multimodalâ€‘MÂ Repository

Welcome! This submission hosts **M1â€¯â†’â€¯M4** multimodal Transformer experiments on the Pennâ€‘Action dataset, plus tooling for data preparation, training, evaluation, and visualisation.

## ðŸ“‚ Directory layout

| Path                | What youâ€™ll find                                                                                                                                                                     | Further docs                                |
| ------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------- |
| **`data_loaders/`** | Shared factories (`create_penn_action_masked_dataloader*.py`) + one adapter folder for each model (**M1â€“M4**) that maps its specific preâ€‘processing to the common dataset interface. | `data_loaders/README_dataloaders.md`        |
| **`enhancements/`** | Qualityâ€‘ofâ€‘life patches added after M3: safer decoder masking, training watchdogs, oneâ€‘batch smoke test, etc.                                                                        | `enhancements/README_enhancements.md`       |
| **`models/`**       | Training notebooks, Hydra configs, and checkpoints for each revision. Highlights: **M4** (14â€¯hâ€¯34â€¯m runâ€‘time, best generalisation) with full README.                                  | `models/README_M4.md`, plus perâ€‘model notes |
| **`results/`**      | Qualitative outputs. Currently stores **6â€‘frame videos** generated by the **M2** modelâ€”handy for eyeballing temporal coherence.                                                      | *selfâ€‘contained*                            |
| **`data_viewing/`** | Interactive notebook to inspect raw frames, skeletons sideâ€‘byâ€‘side.                                                      | *inline docs*                               |
