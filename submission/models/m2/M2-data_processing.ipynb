{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd78aada-47b6-470c-b472-e128aad29e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Penn-Action → fourM preprocessing helpers\n",
    "========================================\n",
    "Execute this cell once.  It defines functions but does **no I/O** until you call\n",
    "`run(...)` from Cell 2.\n",
    "\n",
    "**Output schema (per video)**\n",
    "```\n",
    "<OUTPUT_ROOT>/\n",
    "  tok_rgb/        <video>/<ref>.npy          – Cosmos tokens of reference frame\n",
    "  coords/         <video>/<ref>.npy          – 3×13 (x_tok, y_tok, vis)\n",
    "  captions/       <video>/<ref>.json         – BLIP caption\n",
    "\n",
    "  tok_rgb_next/   <video>/<ref>_n1.npy       – Cosmos tokens of next‑frame #1 …\n",
    "  coords_next/    <video>/<ref>_n1.npy       – joint tokens of next‑frame #1 …\n",
    "```\n",
    "Every RGB token file has a **matching coords file**.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from huggingface_hub import snapshot_download\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "\n",
    "# ------------------------ constants ------------------------\n",
    "TARGET_RES       = 256     # shorter side after resize for Cosmos encoder\n",
    "SSIM_THRESHOLD   = 0.89   # duplicate‑frame threshold (SSIM > thr ⇒ skip)\n",
    "REF_PLUS_NEXT    = 7      # 1 reference + n distinct successor frames\n",
    "COORD_QLEVELS    = 8192    # discrete bins for x & y\n",
    "\n",
    "# ------------------------ similarity ------------------------\n",
    "\n",
    "def is_similar(img1: Image.Image, img2: Image.Image, thr: float = SSIM_THRESHOLD) -> bool:\n",
    "    a = np.asarray(img1.convert(\"L\"), dtype=np.float32)\n",
    "    b = np.asarray(img2.convert(\"L\"), dtype=np.float32)\n",
    "    return ssim(a, b, data_range=255.0) > thr\n",
    "\n",
    "# ------------------------ tokenisers ------------------------\n",
    "\n",
    "def build_tokenisers(device: str):\n",
    "    ckpt_dir = Path(\"/tmp/cosmos_DI16x16\")\n",
    "    if not ckpt_dir.exists():\n",
    "        snapshot_download(\"nvidia/Cosmos-0.1-Tokenizer-DI16x16\", local_dir=str(ckpt_dir))\n",
    "    from cosmos_tokenizer.image_lib import ImageTokenizer\n",
    "\n",
    "    image_tok = ImageTokenizer(\n",
    "        checkpoint_enc=str(ckpt_dir / \"encoder.jit\"),\n",
    "        checkpoint_dec=str(ckpt_dir / \"decoder.jit\"),\n",
    "    ).to(device).eval()\n",
    "\n",
    "    proc = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device).eval()\n",
    "    return image_tok, proc, blip\n",
    "\n",
    "# ------------------------ helpers ------------------------\n",
    "\n",
    "def encode_rgb(img: Image.Image, image_tok, device: str) -> np.ndarray:\n",
    "    ten = TF.to_tensor(img).to(device).unsqueeze(0) * 2 - 1\n",
    "    with torch.no_grad():\n",
    "        tok, _ = image_tok.encode(ten)\n",
    "    return tok.squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "def caption_image(img: Image.Image, proc, blip, device: str) -> str:\n",
    "    with torch.no_grad():\n",
    "        inp = proc(images=img, return_tensors=\"pt\").to(device)\n",
    "        out = blip.generate(**inp, max_length=30)\n",
    "    return proc.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def quantise_coords(x_px: np.ndarray, y_px: np.ndarray, vis: np.ndarray, img_size: int = TARGET_RES) -> np.ndarray:\n",
    "    \"\"\"Pixel coordinates → discrete tokens.\n",
    "\n",
    "    *Normalises* by `img_size` internally, so callers can pass pixel coordinates\n",
    "    in the resized frame space (0‥img_size). Returns a (3,13) uint16 array.\n",
    "    \"\"\"\n",
    "    q = COORD_QLEVELS - 1\n",
    "    x_tok = np.clip(np.rint(x * q), 0, q).astype(np.uint16)\n",
    "    y_tok = np.clip(np.rint(y * q), 0, q).astype(np.uint16)\n",
    "    vis_tok = vis.astype(np.uint16)\n",
    "    return np.stack([x_tok,y_tok,vis_tok])\n",
    "\n",
    "# ------------------------ core routine ------------------------\n",
    "\n",
    "def process_video(vid: str,\n",
    "                  frames_root: Path,\n",
    "                  labels_root: Path,\n",
    "                  out_root: Path,\n",
    "                  image_tok,\n",
    "                  proc,\n",
    "                  blip,\n",
    "                  device: str):\n",
    "\n",
    "    frame_dir  = frames_root / vid\n",
    "    label_path = labels_root / f\"{vid}.mat\"\n",
    "    if not frame_dir.exists() or not label_path.exists():\n",
    "        print(f\"⚠️  Skip {vid}: missing data\")\n",
    "        return\n",
    "\n",
    "    mat = scipy.io.loadmat(label_path, squeeze_me=True, struct_as_record=False)\n",
    "    x_all, y_all = mat[\"x\"], mat[\"y\"]          # pixel coords in original frame\n",
    "    vis_all     = mat[\"visibility\"].astype(bool)\n",
    "    T           = int(mat[\"nframes\"])\n",
    "    H0, W0, _   = mat[\"dimensions\"]\n",
    "    sx, sy      = TARGET_RES / W0, TARGET_RES / H0   # scale to TARGET_RES canvas\n",
    "\n",
    "    frame_files: List[Path] = sorted(frame_dir.glob(\"*.jpg\"))\n",
    "    assert len(frame_files) == T, f\"{vid}: frame/label mismatch\"\n",
    "\n",
    "    # choose reference + up to 6 visually distinct successors\n",
    "    kept = [0]; last = Image.open(frame_files[0]).convert(\"RGB\")\n",
    "    for j in range(1, T):\n",
    "        if len(kept) == REF_PLUS_NEXT:\n",
    "            break\n",
    "        cand = Image.open(frame_files[j]).convert(\"RGB\")\n",
    "        if not is_similar(last, cand):\n",
    "            kept.append(j); last = cand\n",
    "\n",
    "    ref_idx, future_indices = kept[0], kept[1:]\n",
    "    ref_stem = f\"{ref_idx+1:05d}\"\n",
    "\n",
    "    # output dirs\n",
    "    for sub in (\"tok_rgb\", \"coords\", \"captions\", \"tok_rgb_next\", \"coords_next\"):\n",
    "        (out_root / sub / vid).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # ---- reference ----\n",
    "    ref_img = Image.open(frame_files[ref_idx]).convert(\"RGB\")\n",
    "    ref_res = TF.resize(ref_img, (TARGET_RES, TARGET_RES), interpolation=Image.BICUBIC)\n",
    "    np.save(out_root/\"tok_rgb\"/vid/f\"{ref_stem}.npy\", encode_rgb(ref_res, image_tok, device))\n",
    "\n",
    "    x_px = x_all[ref_idx] * sx; y_px = y_all[ref_idx] * sy\n",
    "    np.save(out_root/\"coords\"/vid/f\"{ref_stem}.npy\", quantise_coords(x_px, y_px, vis_all[ref_idx]))\n",
    "\n",
    "    cap = caption_image(ref_res, proc, blip, device)\n",
    "    with open(out_root/\"captions\"/vid/f\"{ref_stem}.json\", \"w\") as fp:\n",
    "        json.dump({\"video\": vid, \"frame\": ref_stem, \"caption\": cap}, fp, indent=2)\n",
    "\n",
    "    # ---- successors ----\n",
    "    for k, idx in enumerate(future_indices, 1):\n",
    "        stem_next = f\"{ref_stem}_n{k}\"\n",
    "        img = Image.open(frame_files[idx]).convert(\"RGB\")\n",
    "        img_res = TF.resize(img, (TARGET_RES, TARGET_RES), interpolation=Image.BICUBIC)\n",
    "        np.save(out_root/\"tok_rgb_next\"/vid/f\"{stem_next}.npy\", encode_rgb(img_res, image_tok, device))\n",
    "\n",
    "        x_px = x_all[idx] * sx; y_px = y_all[idx] * sy\n",
    "        np.save(out_root/\"coords_next\"/vid/f\"{stem_next}.npy\", quantise_coords(x_px, y_px, vis_all[idx]))\n",
    "\n",
    "    print(f\"✅ {vid}: reference + {len(future_indices)} next frames written → {out_root}\")\n",
    "\n",
    "# ------------------------ wrapper ------------------------\n",
    "\n",
    "def run(frames_root: Path,\n",
    "        labels_root: Path,\n",
    "        output_root: Path,\n",
    "        video: Optional[str] = None):\n",
    "    assert frames_root.exists(), \"frames_root missing\"\n",
    "    assert labels_root.exists(), \"labels_root missing\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_tok, proc, blip = build_tokenisers(device)\n",
    "\n",
    "    vids = [video] if video else sorted([d.name for d in frames_root.iterdir() if d.is_dir()])\n",
    "    for vid in tqdm(vids, desc=\"videos\"):\n",
    "        process_video(vid, frames_root, labels_root, output_root, image_tok, proc, blip, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a942a8a-d1d8-439f-93cb-023043204fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Execute **after** Cell 1.\n",
    "Edit the three paths and (optionally) `VIDEO_ID`, then run the cell to start\n",
    "pre‑processing.\n",
    "\"\"\"\n",
    "\n",
    "# --- user‑editable paths ---------------------------------\n",
    "FRAMES_ROOT = Path(\"/home/skalli/COM-304-FM/project/penn_action_raw/Penn_Action/frames\")   \n",
    "LABELS_ROOT = Path(\"/home/skalli/COM-304-FM/project/penn_action_raw/Penn_Action/labels\")  \n",
    "OUTPUT_ROOT = Path(\"/home/skalli/COM-304-FM/project/new/output/\")                     \n",
    "VIDEO_ID    = None  # e.g. \"0003\" for a single clip\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "run(FRAMES_ROOT, LABELS_ROOT, OUTPUT_ROOT, VIDEO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a40d8-5635-45c7-87d9-83c559a0e191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_output_dataset(\n",
    "    output_root: Path,\n",
    "    train_ratio: float = 0.8,\n",
    "    eval_ratio: float = 0.1,\n",
    "    test_ratio: float = 0.1,\n",
    "    seed: int = 42\n",
    "):\n",
    "    assert abs(train_ratio + eval_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n",
    "\n",
    "    modalities = [\"tok_rgb\", \"coords\", \"captions\", \"tok_rgb_next\", \"coords_next\"]\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Find all pair IDs from one modality (they should be the same across all)\n",
    "    base_dir = output_root / \"tok_rgb\"\n",
    "    all_ids = [d.name for d in base_dir.iterdir() if d.is_dir() and d.name.isdigit()]\n",
    "    all_ids.sort()\n",
    "    random.shuffle(all_ids)\n",
    "\n",
    "    N = len(all_ids)\n",
    "    n_train = int(N * train_ratio)\n",
    "    n_eval = int(N * eval_ratio)\n",
    "\n",
    "    splits = {\n",
    "        \"train\": all_ids[:n_train],\n",
    "        \"eval\":  all_ids[n_train:n_train + n_eval],\n",
    "        \"test\":  all_ids[n_train + n_eval:]\n",
    "    }\n",
    "\n",
    "    print(f\"Found {N} pairs: {len(splits['train'])} train, {len(splits['eval'])} eval, {len(splits['test'])} test\")\n",
    "\n",
    "    for split_name, split_ids in splits.items():\n",
    "        for pid in split_ids:\n",
    "            for mod in modalities:\n",
    "                src = output_root / mod / pid\n",
    "                dst = output_root / split_name / mod / pid\n",
    "                if src.exists():\n",
    "                    dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    shutil.move(str(src), str(dst))\n",
    "\n",
    "    print(\"✅ Split completed. Original folders now empty (only train/eval/test remain).\")\n",
    "\n",
    "# Example usage:\n",
    "split_output_dataset(Path(\"output\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
