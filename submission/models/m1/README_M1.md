# M1 Data Processing & Training Report

## ğŸ““ Notebook Index
| Notebook / File | Purpose |
|-----------------|---------|
| **data_penn_view.ipynb** | Quick sanityâ€‘check: loads a *masked* batch through the custom dataloader and verifies that everything compiles on GPU. |
| **data_split.ipynb** | Performs the 70â€¯/â€¯15â€¯/â€¯15 split (train / eval / test) and moves samples into their respective folders with a fixed seed for reproducibility. |
| **data_tokenize_rgb.ipynb** | Demonstrates Cosmos tokenisation for **one RGB frame** (resize â†’Â tensor â†’Â Cosmosâ€‘0.1 DI16Ã—16). |
| **data_tokenize_caption.ipynb** | Uses **SalesforceÂ BLIP (base)** to caption a single frame and write the result to JSON. |
| **data_tokenize_humanâ€‘poses.ipynb** | Explores two strategies for humanâ€‘pose tokens (canvas vs. quantised). After TA feedback the **quantisedâ€‘coords** route was chosen. |
| **data_tokenize_all_example.ipynb** | Endâ€‘toâ€‘end pipeline on **one example** to confirm that all three modalities line up correctly. |
| **data_tokenize_all.ipynb** | Full dataset processing: for **every frame of every video** it writes `tok_rgb`, `coords`, and `captions` so each sample is selfâ€‘contained. |
| **m1-config.yaml** (renamed from *multi_penn_action.yaml*) | Complete training configuration used in this run. |
| **M1.ipynb** | Postâ€‘training metrics, qualitative sampling, and loss curves. |

---

## ğŸ§© Tokenisation Overview

| Modality   | What it represents                         | How it is tokenised |
|------------|--------------------------------------------|---------------------|
| `tok_rgb`  | **RGB frame tokens** (reference & targets) | 1) Resize shorter side to **256â€¯px**.<br>2) Scale to [-1,Â 1].<br>3) **Cosmosâ€‘0.1 DI16Ã—16** â†’Â 1024 discrete tokens. |
| `coords`   | **13â€‘joint human pose** of the frame       | Normalise each joint to the 256â€¯Ã—â€¯256 canvas and **quantise (0â€“8191)** both *x* and *y*. Visibility is kept as a binary token â†’Â (3Â Ã—Â 13) output. |
| `captions` | **Naturalâ€‘language caption**               | Generated with **BLIP (base)** and stored as raw text. |

---

## ğŸ”€ Data Splitting
The dataset is shuffled with `seedÂ =Â 42` and split **70â€¯% train / 15â€¯% eval / 15â€¯% test** using `data_split.ipynb`.  
Every sample contains the three modalities above so models can be trained *causally* or *multitask* without extra I/O.

---

## âš™ï¸ Training Configuration (see `m1-config.yaml`)
```yaml
# Core modal settings
modalities: ["tok_rgb", "coords", "captions"]
vocab_sizes: [65536, 8192, 65536]
max_seq_lens: [256, 39, 64]   # 13 joints Ã—3 â‰ˆ39, caption truncated at 64 tokens
input_alphas:  [1, 1, 1]
target_alphas: [1, 1, 1]

# Selected optimisation hyperâ€‘params
batch_size:  ${global_vars.batch_size}
total_tokens: 5000M
warmup_tokens: 500M
num_tokens_per_sample: 192     # 128Â in + 64Â out  (captions shorter)
lr: 6e-4
min_lr: 1e-6
weight_decay: 0.05
clip_grad: 1.0
dtype: fp32
training_time: 12:29:13
```

---

## ğŸ“‰ Losses (final epoch)
| Component | Loss |
|-----------|------|
| **Total** | **3.70** |
| Captions  | 0.03 |
| Coords    | 2.42 |
| tok_rgb   | 8.89 |

---

## ğŸ§ª Analysis
- **Captions:** Nearâ€‘perfect generation (lossÂ 0.03); text descriptions are coherent and relevant to the visual content.  
- **Coords:** Solid (lossÂ 2.42); predicted joint trajectories follow the groundâ€‘truth motion convincingly.  
- **RGB Frames:** Performance is **poor** â€“ attempts to reconstruct frames from pose â†’Â RGB produce mostly a **black box artefact** with faint silhouettes.  
  - Indicates that the *image space* objective dominates the total loss but fails to converge.  
  - Likely caused by **information asymmetry** (coords â‰ª RGB token count) and insufficient decoder capacity.

### Next Steps
1. **Revise data pipeline** â€“ experiment with *frameâ€‘toâ€‘frame* targets (`tok_rgb_next`) instead of poseâ€‘toâ€‘RGB to reduce modality gap.  
2. **Curriculum learning** â€“ start with lowerâ€‘resolution DI32Ã—32 tokens and progressively unfreeze higher frequencies.  
3. **Loss reâ€‘balancing** â€“ scale the RGB term down or introduce *perceptual* / *VQGAN* losses.  
4. **Augment captions** â€“ add action verbs (BLIPÂ Ã—Â ActionBank) to help align motion semantics.

---
