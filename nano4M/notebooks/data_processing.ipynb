{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ba3dad-2d03-40f2-8bd3-4f0545cb590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Penn‑Action → fourM preprocessing helpers\n",
    "========================================\n",
    "Put this cell at the top of your notebook; execute it once.  It defines every\n",
    "function you need **but does nothing on disk** until you call `run(...)` from\n",
    "Cell 2.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import torch\n",
    "import torchvision.transforms.functional as TF\n",
    "from huggingface_hub import snapshot_download\n",
    "from PIL import Image\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from tqdm import tqdm\n",
    "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
    "\n",
    "# ------------------------ constants ------------------------\n",
    "TARGET_RES = 256          # shorter side after resize for Cosmos encoder\n",
    "SSIM_THRESHOLD = 0.985    # duplicate‑frame threshold (SSIM > thr ⇒ skip)\n",
    "MAX_FRAMES = 6            # 1 reference + 5 distinct successors\n",
    "COORD_QLEVELS = 8192      # discrete bins for x & y\n",
    "\n",
    "# ------------------------ similarity ------------------------\n",
    "\n",
    "def is_similar(img1: Image.Image, img2: Image.Image, thr: float = SSIM_THRESHOLD) -> bool:\n",
    "    \"\"\"Return True if two RGB PIL images are perceptually similar.\"\"\"\n",
    "    a = np.asarray(img1.convert(\"L\"), dtype=np.float32)\n",
    "    b = np.asarray(img2.convert(\"L\"), dtype=np.float32)\n",
    "    score, _ = ssim(a, b, full=True)\n",
    "    return score > thr\n",
    "\n",
    "# ------------------------ tokenisers ------------------------\n",
    "\n",
    "def build_tokenisers(device: str):\n",
    "    \"\"\"Download (once) and load Cosmos + BLIP models.\"\"\"\n",
    "    ckpt_dir = Path(\"/tmp/cosmos_DI16x16\")\n",
    "    if not ckpt_dir.exists():\n",
    "        snapshot_download(\"nvidia/Cosmos-0.1-Tokenizer-DI16x16\", local_dir=str(ckpt_dir))\n",
    "    from cosmos_tokenizer.image_lib import ImageTokenizer  # deferred import\n",
    "\n",
    "    image_tok = ImageTokenizer(\n",
    "        checkpoint_enc=str(ckpt_dir / \"encoder.jit\"),\n",
    "        checkpoint_dec=str(ckpt_dir / \"decoder.jit\"),\n",
    "    ).to(device)\n",
    "    image_tok.eval()\n",
    "\n",
    "    proc = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "    blip = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
    "    blip.eval()\n",
    "\n",
    "    return image_tok, proc, blip\n",
    "\n",
    "# ------------------------ helpers ------------------------\n",
    "\n",
    "def encode_rgb(img: Image.Image, image_tok, device: str) -> np.ndarray:\n",
    "    ten = TF.to_tensor(img).unsqueeze(0).to(device) * 2 - 1\n",
    "    with torch.no_grad():\n",
    "        tok, _ = image_tok.encode(ten)\n",
    "    return tok.squeeze(0).cpu().short().numpy()\n",
    "\n",
    "\n",
    "def caption_image(img: Image.Image, proc, blip, device: str) -> str:\n",
    "    with torch.no_grad():\n",
    "        inputs = proc(images=img, return_tensors=\"pt\").to(device)\n",
    "        out = blip.generate(**inputs, max_length=30)\n",
    "    return proc.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def quantise_coords(x: np.ndarray, y: np.ndarray, vis: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Return (3,13) uint16 array of x, y, visibility tokens.\"\"\"\n",
    "    q = COORD_QLEVELS - 1\n",
    "    x_tok = np.clip(np.rint(x * q), 0, q).astype(np.uint16)\n",
    "    y_tok = np.clip(np.rint(y * q), 0, q).astype(np.uint16)\n",
    "    vis_tok = vis.astype(np.uint16)\n",
    "    return np.stack([x_tok, y_tok, vis_tok])\n",
    "\n",
    "# ------------------------ core routine ------------------------\n",
    "\n",
    "def process_video(\n",
    "    vid: str,\n",
    "    frames_root: Path,\n",
    "    labels_root: Path,\n",
    "    out_root: Path,\n",
    "    image_tok,\n",
    "    proc,\n",
    "    blip,\n",
    "    device: str,\n",
    "):\n",
    "    \"\"\"Tokenise one Penn‑Action clip.\"\"\"\n",
    "    frame_dir = frames_root / vid\n",
    "    label_path = labels_root / f\"{vid}.mat\"\n",
    "    if not frame_dir.exists() or not label_path.exists():\n",
    "        print(f\"⚠️  Skip {vid}: missing data\")\n",
    "        return\n",
    "\n",
    "    mat = scipy.io.loadmat(label_path, squeeze_me=True, struct_as_record=False)\n",
    "    x_all, y_all = mat[\"x\"], mat[\"y\"]\n",
    "    vis_all = mat[\"visibility\"].astype(bool)\n",
    "    T = int(mat[\"nframes\"])\n",
    "    H0, W0, _ = mat[\"dimensions\"]\n",
    "    scale_x, scale_y = TARGET_RES / W0, TARGET_RES / H0\n",
    "\n",
    "    frame_files: List[Path] = sorted(frame_dir.glob(\"*.jpg\"))\n",
    "    assert len(frame_files) == T, f\"{vid}: frame/label mismatch\"\n",
    "\n",
    "    # select MAX_FRAMES distinct frames\n",
    "    kept = [0]\n",
    "    last = Image.open(frame_files[0]).convert(\"RGB\")\n",
    "    for j in range(1, T):\n",
    "        if len(kept) == MAX_FRAMES:\n",
    "            break\n",
    "        cand = Image.open(frame_files[j]).convert(\"RGB\")\n",
    "        if not is_similar(last, cand):\n",
    "            kept.append(j)\n",
    "            last = cand\n",
    "\n",
    "    for sub in (\"tok_rgb\", \"tok_rgb_next\", \"coords\", \"captions\"):\n",
    "        (out_root / sub / vid).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for idx in kept:\n",
    "        stem = f\"{idx+1:05d}\"\n",
    "        img = Image.open(frame_files[idx]).convert(\"RGB\")\n",
    "        img_res = TF.resize(img, TARGET_RES, interpolation=Image.BICUBIC)\n",
    "        np.save(out_root / \"tok_rgb\" / vid / f\"{stem}.npy\", encode_rgb(img_res, image_tok, device))\n",
    "\n",
    "        # next frame\n",
    "        if idx + 1 < T:\n",
    "            nxt = Image.open(frame_files[idx+1]).convert(\"RGB\")\n",
    "            nxt_res = TF.resize(nxt, TARGET_RES, interpolation=Image.BICUBIC)\n",
    "            np.save(out_root / \"tok_rgb_next\" / vid / f\"{stem}.npy\", encode_rgb(nxt_res, image_tok, device))\n",
    "\n",
    "        # coords\n",
    "        x_n = (x_all[idx] * scale_x) / TARGET_RES\n",
    "        y_n = (y_all[idx] * scale_y) / TARGET_RES\n",
    "        np.save(out_root / \"coords\" / vid / f\"{stem}.npy\", quantise_coords(x_n, y_n, vis_all[idx]))\n",
    "\n",
    "        # caption\n",
    "        cap = caption_image(img_res, proc, blip, device)\n",
    "        with open(out_root / \"captions\" / vid / f\"{stem}.json\", \"w\") as fp:\n",
    "            json.dump({\"video\": vid, \"frame\": stem, \"caption\": cap}, fp, indent=2)\n",
    "\n",
    "    print(f\"✅ {vid}: {len(kept)} frames → {out_root}\")\n",
    "\n",
    "# ------------------------ convenience wrapper ------------------------\n",
    "\n",
    "def run(\n",
    "    frames_root: Path,\n",
    "    labels_root: Path,\n",
    "    output_root: Path,\n",
    "    video: Optional[str] = None,\n",
    "):\n",
    "    \"\"\"High‑level entry point you invoke from Cell 2.\"\"\"\n",
    "    assert frames_root.exists(), \"frames_root path does not exist\"\n",
    "    assert labels_root.exists(), \"labels_root path does not exist\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    image_tok, proc, blip = build_tokenisers(device)\n",
    "\n",
    "    vids = [video] if video else sorted([d.name for d in frames_root.iterdir() if d.is_dir()])\n",
    "    for vid in tqdm(vids, desc=\"videos\"):\n",
    "        process_video(vid, frames_root, labels_root, output_root, image_tok, proc, blip, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7820df1d-cb68-494e-bb41-a1348c668722",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "frames_root path does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m VIDEO_ID    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0001\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# e.g. \"0003\" for a single clip\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFRAMES_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mLABELS_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVIDEO_ID\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 159\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(frames_root, labels_root, output_root, video)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun\u001b[39m(\n\u001b[1;32m    153\u001b[0m     frames_root: Path,\n\u001b[1;32m    154\u001b[0m     labels_root: Path,\n\u001b[1;32m    155\u001b[0m     output_root: Path,\n\u001b[1;32m    156\u001b[0m     video: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    157\u001b[0m ):\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"High‑level entry point you invoke from Cell 2.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m frames_root\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframes_root path does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m labels_root\u001b[38;5;241m.\u001b[39mexists(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels_root path does not exist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    162\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: frames_root path does not exist"
     ]
    }
   ],
   "source": [
    "\"\"\"Execute **after** Cell 1.\n",
    "Edit the three paths and (optionally) `VIDEO_ID`, then run the cell to start\n",
    "pre‑processing.\n",
    "\"\"\"\n",
    "\n",
    "# --- user‑editable paths ---------------------------------\n",
    "FRAMES_ROOT = Path(\"../COM-304-FM/project/penn_action_raw/Penn_Action/frames\")   # ← change me\n",
    "LABELS_ROOT = Path(\"../COM-304-FM/project/penn_action_raw/Penn_Action/labels\")   # ← change me\n",
    "OUTPUT_ROOT = Path(\"../new/output/\")                      # ok to leave as‑is\n",
    "VIDEO_ID    = \"0001\"  # e.g. \"0003\" for a single clip\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "run(FRAMES_ROOT, LABELS_ROOT, OUTPUT_ROOT, VIDEO_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecec6b1-1fea-43ea-8ce2-8c0ccbf2f1be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
